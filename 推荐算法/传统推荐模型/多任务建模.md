### MoE （混合专家模型）

**概括**：由多个专家和一个门控网络组成。门控网络首先根据输入特征对每个专家进行评估，生成权重或概率分布，然后根据这些权重选择部分或全部专家参与计算，并对专家输出进行加权融合，得到最终预测结果。由于不同建模目标关注的特征和侧重点不同，每个专家可以专注于特定模块，例如冷启动、短期兴趣或长期兴趣建模。门控机制根据训练目标动态选择合适的专家，从而提升模型容量，并能够组合出更深、更复杂的特征表示。

### MMoE （多门控混合专家模型）

**概括**：是 MoE 在多任务场景下的扩展，主要用于同时预测多个目标，比如 CTR、转化率或 CVR。在 MMoE 中，所有任务共享一组专家网络，但每个任务都有自己独立的门控网络。门控会根据输入特征自动评估各个专家的重要性，并选择合适的专家组合进行加权融合，生成当前任务的预测结果。这样，每个任务既可以复用共享专家的知识，又能根据自身需求选择最相关的专家，实现多任务。同时又保持MoE 的复杂模型表达能力，让模型在有限参数量下完成多个任务的预测，很适合推荐系统等多目标场景。

### PLE（渐进分层提取网络）

**概括**：MMoE的专家对所有门控可见，会导致负迁移问题，不同的任务如果相互冲突，都会反向传播影响到某个共同参与的专家，某些专家也会高度复用而某些专家用到的时间很低。并且如果专家数量变多，各种专家中混杂了很多信息，门控需要去筛选，也会影响门控的决策。
PLE对专家在进行分组，分成了共享专家和任务专家，每个任务都拥有专属的专家组，仅需要学习该任务特有的知识或模式。能够有效的解决了负迁移问题和决策难度。

### ESMM（完整空间多任务模型）

**概括**：ESMM主要用于解决CVR任务中训练样本与预测样本分布不一致的问题。CVR模型通常只在点击样本上训练，但需要在全量曝光样本上预测，这会导致模型泛化能力下降，同时点击样本稀疏也限制了训练效果，容易欠拟合。
ESMM通过联合训练CTR和CVR来缓解这一问题。利用公式 CTCVR=CTR×CVR，让模型在全量曝光样本上学习CTCVR。具体来说，CTR和CVR共享底层网络，通过CTR任务在全量样本上学习曝光信息，再结合点击样本学习CVR，从而实现知识迁移，使得CVR得到更多的训练机会和更加稳定有效的预测。

### ESM2（增强型完整空间多任务模型）

**概括**：ESM² 相比 ESMM 的关键提升，不只是加深网络层数或增加参数，而是对点击之后的行为链进行了显式分解。具体来说，它把用户在点击后的潜在行为分为多类中间行为，例如 Deterministic Action（D Action，比如加购、愿望单） 和 Other Action（O Action，其他转化相关行为），并将这些中间行为作为独立的子预测任务。
在训练过程中，ESM²在整个曝光样本空间上对所有子任务进行联合优化，每个子任务都学习从曝光→点击→中间行为→购买的条件概率。最终模型通过条件概率链将这些子任务的输出组合，得到最终 CVR 预测。
这种做法的好处是：
缓解数据稀疏问题：通过中间行为提供更多监督信号，即使最终购买样本很少，也能学习到有效模式。
消除样本选择偏差：每个子任务都在曝光样本空间上训练，不再仅依赖点击样本

