### DIN（深度兴趣网络）

**口语概括**：在以前的做法中，用户的兴趣向量往往是基于用户历史行为的物品作平均pooling。这种固定长度的向量因为承载的信息众多，反而不易于聚焦于具体的推荐任务。打个比方，比如用户想不想去买一个手机壳，往往是因为他购买了什么样的手机来决定的，跟他买了个什么样的枕头、吹风机联系不大。所以DIN提出了一个观点，用户的兴趣向量不应该固定的，而是应该根据候选物品而动态变化的，用户是否购买当前候选物品往往由历史行为中的部分片段来决定的，这一部分贡献最大。
所以借助自注意力机制来实现，对每个候选物品分别对用户行为序列作attention，计算出每个历史行为的注意力分数，也就是注意力占比，经过softmax得到我们的注意力权重，加权的生成针对该物料的用户兴趣向量，再把用户、用户兴趣向量、物品向量、上下文特征拼接到一下送入mlp，经过sigmoid做预测CTR，会得到更加精准的结果。

**Loss: **

**CTR预测**：BCE 二元交叉熵损失函数

```cpp
import numpy as np

def binary_cross_entropy(predictions, targets, epsilon=1e-12):
    """
    手动实现二元交叉熵损失函数
    
    参数:
    predictions: 预测概率，范围(0,1)
    targets: 真实标签，值为0或1
    epsilon: 防止log(0)的小常数
    """
    # 限制预测值的范围，避免log(0)
    predictions = np.clip(predictions, epsilon, 1. - epsilon)
    
    # 计算二元交叉熵
    bce = -targets * np.log(predictions) - (1 - targets) * np.log(1 - predictions)
    
    # 返回平均损失
    return np.mean(bce)
```

**回归任务：**MSE均方误差

```cpp
import numpy as np

def mse_basic(y_true, y_pred):
    """
    计算均方误差（MSE）
    
    参数:
    y_true : array-like, 真实值
    y_pred : array-like, 预测值
    
    返回:
    float, 均方误差
    """
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    
    if len(y_true) != len(y_pred):
        raise ValueError("y_true 和 y_pred 的长度必须相同")
    
    squared_errors = (y_true - y_pred) ** 2
    mse = np.mean(squared_errors)
    return mse
```

### DIEN（深度兴趣进化网络）

**概括**：DIN本质上还是把用户历史行为当成一个静态的，候选物品对每个历史物品求注意力权重，这个每个历史行为被考虑到的机会都是均等的。
DIEN进一步认为用户兴趣向量本身是一个随时间演化的过程，通过GRU去顺序的建模用户行为序列，将用户的每一次历史行为映射为兴趣状态的一次更新。GRU包括更新门和重置门，将用户历史行为序列转化为一个融合了历史记忆的隐向量序列。再利用AUGRU像DIN那样将用户兴趣片段隐向量去跟候选物品作attention得到注意力权重，最终加权求得注意力分数，经过sigmoid预测CTR