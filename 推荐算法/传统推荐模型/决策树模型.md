### GBDT

**概括**：梯度决策提升树。是一种逐步拟合残差、通过梯度提升优化损失、累加多棵决策树输出逐步逼近真实值的树模型。通过超参数比如限制树的最大深度和叶子结点最大数量，按照层级优先的策略生长，没有像前剪枝和后剪枝这样的剪枝策略，尽量生长到限制条件为止

### XGBoost

**概括**：基于梯度提升的集成学习算法，通过二阶泰勒展开近似损失函数来计算叶子结点的权重和分裂增益，从而逐步拟合残差优化预测。它支持特征级和数据级并行，可进行分布式服务器训练。树的生长采用按层生长策略，结合预剪枝和后剪枝，同时限制最大深度和叶子节点数，并且引入了L2正则项，用来限制树的结构或分支无限增长，使得模型更加稳定的更快，适合大规模数据处理。

**（1）疑问：为什么要后剪枝？怎么后剪枝？**
后剪枝就是在树生长完毕以后再回头查看分支贡献过小或增益不够，再选择把它们剪掉，也是采用阈值的方式进行筛选，能让树结构更加简洁
**（2）疑问：XGBoost 和 GBDT的区别在哪里？**
emm，树的限制策略，多了剪枝的正则化。梯度计算中也通过泰勒展开加入了二阶梯度。并且支持特征级数据并行，数据集也可以分布式训练再整合。而GBDT是严格串行的。
XGBoost还引入了预排序，把每个特征的值排序并缓存起来。这样可以通过前缀和的方式快速地计算左子树的梯度和还有右子树的梯度和

### LightGBM

概括：LightGBM是基于GBDT的一个树模型。首先将连续的特征通过直方图算法离散化的进行分桶操作，从而在每个叶子节点中以bin为单位累加一阶梯度和二阶梯度。在寻找分裂时，每个叶子节点都维护了一张以特征为行，以分桶的桶数量为列的梯度直方图矩阵，然后选择分裂增益最大的特征和切分点作为第一次if-else的分裂。每次都是对所有的叶子节点遍历，寻找分裂增益最大的叶子结点进行分裂。因为叶子权重都是通过目标函数的二阶泰勒展开直接解析计算的，一旦确定就不能再调整。因此，单棵树的拟合是有限的，所以需要训练多棵树，每一棵树都去拟合前面模型与真实值的残差，最终累加所有树的输出，逐步逼近真实值。
还有一些特性，比如特征捆绑和GOSS单边采样，处理大批量数据集时会保留全部梯度大的样本，而对梯度小的样本进行随机采样。在开始训练前选择样本。

**（1）疑问：你一直在说分裂的增益最大的？这个增益是什么？如何量化？**
LightGBM的损失函数是交叉熵损失函数。在每个叶子里面，计算损失函数对当前预测值的一阶梯度和二阶梯度，然后用这些梯度去计算最优输出。
对于分裂增益，分别计算左右子叶分裂后的损失相加，减去分裂惩罚和父节点原损失，得到负值的净增量就是他的增益。
**（2）疑问：多棵树最终的累加，是直接累加的，还是基于可学习的权重的加权累加？**
每棵树训练前都会有一个超参数学习率，每棵树输出 * 学习率做加权累加 。这里把在初始化的时候，把所有的树的学习率都调成了0.01
**（4）疑问：学习率大的会怎么样？小了会怎么样？**
越大修正幅度更大，收敛越快，但是容易过拟合。越小收敛越慢，需要更多的树来模拟残差，但稳定性和泛化性更好
**（4）疑问：Leaf-wise更激进，如何被管住呢？**
因为它是基于一个局部最优，也就是贪心的思想来分裂。所以我们要引入约束和L2正则化。一方面通过最大深度和最大叶子数来限制树的结构复杂度防止某一分支无限生长；一方面呢是对损失引入了L2正则，对叶子权重过大、贡献集中的热门分支施加更强的惩罚，使得边际收益快速下降，反而对生长较慢的权重较小的分支影响较弱，相对的鼓励其分裂。

**LightGBM排序模型 和LightGBM 分类模型的区别**
LightGBM分类模型预测每个样本的类别概率，关注样本与真实标签的差距；排序模型关注同一组候选样本的相对顺序，训练时损失函数会考虑样本间的排序关系，输出是用于排序的分数而不是概率。分类用的交叉熵损失函数，排序模型用的是排序损失lambdaRank

**LightGBM排序模型的损失**
排序模型是用的默认的lambdarank。他只会关心同一group下的item顺序是否正确。不直接定义一个具体的损失函数，而是为每个item pair生成一个梯度λ。使得模型在考虑物品对相对顺序的同时，直接针对NDCG列表级评价指标进行优化。
λ= |δNDCGij|*1/1 + e(si - sj)
如果交换ij的排名，NDCG的变化量，越重要的错误权重越大