## vLLM

模型的**高性能推理与服务部署**，通过优化显存缓存机制，大幅提升缓存利用率和整体推理效率，同时支持服务端多并发、多用户交互，是目前业界做 LLM 在线服务最主流的推理引擎之一。

------

#### 技术核心

- **PagedAttention（分页策略）** 
  借鉴了操作系统虚拟内存分页思想，将 KV 缓存拆为固定大小的物理页（最小显存分配单元），通过页表映射离散物理页与请求的逻辑 KV 空间，无需连续显存，仅分配实际需要的页。提升了显存利用率，并发GPU请求任务数提高

- **Continuous Batching（动态批处理调度策略）**

  - **传统痛点**

    **静态批处理**：要等凑够固定数量的请求才开始计算，并且整批请求必须**同步生成到最后**，快请求必须等慢请求，导致 GPU 大量空转、利用率极低。
    **批次大小固定**：遇到突发流量时弹性差，新请求只能排队等待，很容易出现延迟高、排队超时。整体吞吐量上不去，快请求被慢请求拖慢，用户体验差，GPU 资源无法被充分利用。

  - **动态批处理调度策略**
    GPU 不再等整批请求完成，而是 “见缝插针” 处理所有未完成的请求，生成一个 token 就推进一个请求的进度，完成即移出，新请求随时加入。（**随时待命，即进即出**）

------

#### KV cache

 Transformer 注意力机制在 LLM 推理阶段，长序列逐 token 生成时 KV 会重复计算，对已生成的每个 token，其在各层注意力层的 K（键）和 V（值）只需计算一次并缓存下来，后续生成新 token 时，仅需计算新 token 的 Q（查询），复用缓存的 KV。用空间换时间。

------

#### FlashAttention

##### 1. 技术背景

传统注意力计算需先生成QKt中间矩阵（大小为n×n，n为序列长度），再做 softmax 和乘 V 运算：

- **访存瓶颈**：Q/K/V 需在 GPU 高速缓存 和 显存 间频繁搬运，GPU 大部分时间耗在数据移动而非计算上；
- **显存爆炸**：长序列下QKT矩阵显存占用呈平方级增长，无法支持超长文本处理；
- **计算效率低**：多步计算需多次读写显存，中间结果存储进一步加剧显存压力。

**2. 技术核心**

- **分块计算**

  LLM 场景下的 Q/K/V 矩阵是 “千万级甚至亿级” 的，直接计算完整矩阵会瞬间占满显存，导致阻塞；
  把大矩阵的计算拆成多个小矩阵的计算，每个小矩阵在 SRAM 里完成计算，结果直接累加到最终输出

- **计算融合**
  将 “QKT计算→softmax→乘 V” 三步操作融合到**单个 CUDA 内核**中，所有中间计算在 SRAM / 寄存器完成，仅将最终结果写回显存，彻底消除中间结果的显存读写。

- **数据重排**

  静态优化 Q/K/V 的显存存储布局，将分块按 “连续地址” 排列，利用空间局部性让显存读取时一次性拿到整个分块，提升带宽利用率（仅静态布局优化，非动态）。

**3. 技术细节**

FlashAttention 的理论计算复杂度和传统注意力机制完全一致，都是O(n^2d)（n为序列长度，d为隐藏层维度）；但它通过优化访存逻辑，把实际运行时的 “访存复杂度” 从O(n^2d)降到了O(nd)，这才是它性能提升的核心。

------

#### FlashAttention_2

| 优化维度         | FlashAttention 1                             | FlashAttention 2                                             |
| :--------------- | -------------------------------------------- | ------------------------------------------------------------ |
| **并行计算粒度** | 仅支持 “单头内分块计算”，多头之间串行处理    | 支持 “多头并行分块计算”，多个注意力头的分块计算同时进行，充分利用 GPU 的 SM 单元 |
| **访存重叠优化** | 计算和访存部分重叠，仍有部分等待             | 完全的 “计算 - 访存流水线”：读取下一个分块的同时计算当前分块，彻底消除访存等待 |
| **硬件适配性**   | 仅适配 NVIDIA Ampere 及以上架构（A100/A800） | 适配更多 GPU 架构（如 Hopper、Ada Lovelace），且支持不同 SRAM 大小的 GPU 动态调整分块大小 |
| **分块策略**     | 固定分块大小（如 256×256），灵活性低         | 动态分块策略：根据 GPU SRAM 容量、序列长度自动调整分块大小，最大化 SRAM 利用率 |
| **额外优化**     | 无                                           | 支持 “双向注意力”（如 GPT 类自回归模型的因果注意力、BERT 类双向注意力）统一优化；减少 CUDA 内核启动开销 |

- **多头并行分块计算：**优化前多头的计算只能占用部分 SM，其余 SM 闲置；优化后将不同注意力头的分块计算任务，分配给不同的 SM 单元同时执行；
- **计算 - 访存流水线：**FlashAttention 2 的 “计算 - 访存流水线” 和 OS 的指令流水线核心思想相同 —— 通过任务拆分和周期调度，让 GPU 的 “计算单元” 和 “访存单元” 始终处于忙碌状态（**提高资源利用率**）
- **动态分块策略：**先算出 SRAM 能容纳的最大尺寸；**再适配序列长度**，把最大尺寸调整为序列长度的约数（优先 2 的幂次）；**最后匹配 SM 并行度**：若分块数量过少（SM 闲置），则适当减小分块尺寸、增加分块数量，直到 SM 单元被充分利用。
- **更高的注意力兼容性**