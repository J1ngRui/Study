## BERT

BERT 本质上是一个使用Transformer Encoder 的文本模型。依靠全自注意力机制 实现上下文的建模。预训练阶段主要通过两个自监督任务学习语言知识：

1.MLM（Masked Language Model）

随机掩盖15% token 并预测原词，其中

- 80% 概率 → 替换成  [MASK]
- 10% 概率 → 替换成随机词
- 10% 概率 → 保持原词不变

仅对 15% Token 计算交叉熵损失函数

2.NSP（Next Sentence Prediction）

判断两个句子是否为连续上下文。

- 50%：IsNext 句子B 真的是句子A的下一句
- 50%：NotNext 句子B 是从语料里随机抽的一句不相关的

但是后期被证明很弱，甚至有害模型，所以可以改用超长句子进行训练。



**BERT的MLM 训练集设计？**

预训练时模型会看到大量 [MASK] 符号， 但在微调和实际推理时，输入文本里完全没有 [MASK]，这就造成了预训练和微调的输入分布不一致。所以 BERT 在 MLM 里采用 80% 替换为 [MASK]、10% 随机词、10% 保持不变的策略，目的就是减轻这种分布不匹配，让模型不要过度依赖 [MASK] 符号， 从而提升泛化能力。